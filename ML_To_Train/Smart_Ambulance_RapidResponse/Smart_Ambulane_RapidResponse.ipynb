{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e53307",
   "metadata": {},
   "outputs": [],
   "source": [
    "Below is a **single clean Jupyter Notebook (baseline model)** you can copy directly into an `.ipynb` (or paste cell-by-cell).\n",
    "It is **production-clean**, consistent scaling, trains per-patient Isolation Forest models, saves them, and includes a **terminal-style live scoring simulation** (1 score per second).\n",
    "\n",
    "No API. No “top anomalies”. Only continuous scoring.\n",
    "\n",
    "---\n",
    "\n",
    "# ✅ Baseline Isolation Forest Notebook (Time-Series Per-Second Scoring)\n",
    "\n",
    "---\n",
    "\n",
    "## Cell 1 — Imports\n",
    "\n",
    "```python\n",
    "import os\n",
    "import globm\n",
    "import time\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Cell 2 — Configuration (edit this)\n",
    "\n",
    "```python\n",
    "# Folder with training CSVs (1 file per patient)\n",
    "TRAIN_DATA_DIR = \"./data/train_patients\"\n",
    "\n",
    "# Where to save models\n",
    "MODEL_DIR = \"./models\"\n",
    "PATIENT_MODEL_DIR = os.path.join(MODEL_DIR, \"patient_models\")\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(PATIENT_MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Columns that must exist in every CSV\n",
    "FEATURE_COLS = [\"heart_rate\", \"spo2\", \"blood_pressure\", \"motion\"]\n",
    "\n",
    "# Isolation Forest hyperparameters (baseline model)\n",
    "ISO_PARAMS = {\n",
    "    \"n_estimators\": 300,\n",
    "    \"contamination\": \"auto\",\n",
    "    \"random_state\": 42,\n",
    "    \"n_jobs\": -1\n",
    "}\n",
    "\n",
    "MIN_TRAIN_ROWS = 50  # skip patient if less than this\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Cell 3 — Utility functions (clean + reusable)\n",
    "\n",
    "```python\n",
    "def list_patient_csvs(folder):\n",
    "    files = sorted(glob.glob(os.path.join(folder, \"*.csv\")))\n",
    "    if len(files) == 0:\n",
    "        raise ValueError(f\"No CSV files found in: {folder}\")\n",
    "    return files\n",
    "\n",
    "\n",
    "def load_patient_df(csv_path, feature_cols):\n",
    "    \"\"\"\n",
    "    Loads one patient CSV, keeps only features, drops NaNs.\n",
    "    Returns a clean DataFrame.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    missing = [c for c in feature_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns in {csv_path}: {missing}\")\n",
    "\n",
    "    df = df[feature_cols].copy()\n",
    "    df = df.dropna()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def patient_id_from_path(path):\n",
    "    return os.path.splitext(os.path.basename(path))[0]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Cell 4 — Discover patient files\n",
    "\n",
    "```python\n",
    "patient_files = list_patient_csvs(TRAIN_DATA_DIR)\n",
    "print(\"Patients found:\", len(patient_files))\n",
    "print(\"Example:\", patient_files[0])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Cell 5 — Fit ONE global scaler (consistent scaling)\n",
    "\n",
    "This ensures the scale is consistent across all patients.\n",
    "\n",
    "```python\n",
    "all_rows = []\n",
    "\n",
    "for f in patient_files:\n",
    "    df = load_patient_df(f, FEATURE_COLS)\n",
    "    if len(df) > 0:\n",
    "        all_rows.append(df)\n",
    "\n",
    "all_rows = pd.concat(all_rows, axis=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(all_rows.values)\n",
    "\n",
    "print(\"Global scaler fitted on shape:\", all_rows.shape)\n",
    "\n",
    "# Save scaler\n",
    "scaler_path = os.path.join(MODEL_DIR, \"global_scaler.pkl\")\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(\"Saved scaler ->\", scaler_path)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Cell 6 — Train per-patient Isolation Forest baseline models\n",
    "\n",
    "```python\n",
    "trained_patients = []\n",
    "\n",
    "for f in patient_files:\n",
    "    pid = patient_id_from_path(f)\n",
    "    df = load_patient_df(f, FEATURE_COLS)\n",
    "\n",
    "    if len(df) < MIN_TRAIN_ROWS:\n",
    "        print(f\"[SKIP] {pid} | rows={len(df)} (< {MIN_TRAIN_ROWS})\")\n",
    "        continue\n",
    "\n",
    "    X_scaled = scaler.transform(df.values)\n",
    "\n",
    "    model = IsolationForest(**ISO_PARAMS)\n",
    "    model.fit(X_scaled)\n",
    "\n",
    "    model_path = os.path.join(PATIENT_MODEL_DIR, f\"{pid}_iforest.pkl\")\n",
    "    joblib.dump(model, model_path)\n",
    "\n",
    "    trained_patients.append(pid)\n",
    "    print(f\"[OK] trained={pid} | rows={len(df)} | saved={model_path}\")\n",
    "\n",
    "print(\"\\nTotal trained patients:\", len(trained_patients))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Cell 7 — Scoring logic (important)\n",
    "\n",
    "Isolation Forest output:\n",
    "\n",
    "* `decision_function`: higher means more normal\n",
    "  We convert it into:\n",
    "* **anomaly_score = -decision_function**\n",
    "  so **higher score = more anomalous**\n",
    "\n",
    "```python\n",
    "def anomaly_scores_from_scaled(model, X_scaled):\n",
    "    \"\"\"\n",
    "    X_scaled: np.ndarray of shape (n, features)\n",
    "    returns: np.ndarray anomaly scores (higher = more anomalous)\n",
    "    \"\"\"\n",
    "    normality = model.decision_function(X_scaled)  # higher = more normal\n",
    "    return -normality\n",
    "\n",
    "\n",
    "def score_one_frame(model, scaler, row_values):\n",
    "    \"\"\"\n",
    "    row_values: list in the same order as FEATURE_COLS\n",
    "    returns: float anomaly score\n",
    "    \"\"\"\n",
    "    x = np.array([row_values], dtype=float)\n",
    "    x_scaled = scaler.transform(x)\n",
    "    return float(anomaly_scores_from_scaled(model, x_scaled)[0])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Cell 8 — Load a trained patient model (for testing)\n",
    "\n",
    "```python\n",
    "if len(trained_patients) == 0:\n",
    "    raise RuntimeError(\"No patients trained. Check your data.\")\n",
    "\n",
    "TEST_PATIENT_ID = trained_patients[0]\n",
    "test_model_path = os.path.join(PATIENT_MODEL_DIR, f\"{TEST_PATIENT_ID}_iforest.pkl\")\n",
    "\n",
    "model = joblib.load(test_model_path)\n",
    "print(\"Loaded patient model:\", TEST_PATIENT_ID)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Cell 9 — Simulate “terminal live scoring” from a CSV (1 row per second)\n",
    "\n",
    "This is exactly the behavior you want:\n",
    "**every second → print score**\n",
    "\n",
    "```python\n",
    "# We'll simulate streaming by reading a CSV row-by-row\n",
    "stream_file = [f for f in patient_files if patient_id_from_path(f) == TEST_PATIENT_ID][0]\n",
    "stream_df = load_patient_df(stream_file, FEATURE_COLS).reset_index(drop=True)\n",
    "\n",
    "print(\"Streaming rows:\", len(stream_df))\n",
    "print(stream_df.head())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Cell 10 — Live scoring loop (prints per second)\n",
    "\n",
    "```python\n",
    "scores = []\n",
    "\n",
    "for t in range(len(stream_df)):\n",
    "    row = stream_df.loc[t].values.tolist()\n",
    "\n",
    "    s = score_one_frame(model, scaler, row)\n",
    "    scores.append(s)\n",
    "\n",
    "    # This print is terminal-style output\n",
    "    print(f\"[t={t:05d}] anomaly_score={s:.5f}\")\n",
    "\n",
    "    time.sleep(1)  # 1 score per second\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Cell 11 — Plot the anomaly score over time (optional)\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(scores)\n",
    "plt.title(f\"Anomaly Score Over Time (Patient: {TEST_PATIENT_ID})\")\n",
    "plt.xlabel(\"Time (seconds / frames)\")\n",
    "plt.ylabel(\"Anomaly Score (higher = more anomalous)\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Cell 12 — Optional: Add alert levels (not top anomalies)\n",
    "\n",
    "This is still continuous scoring — it just gives meaning.\n",
    "\n",
    "```python\n",
    "def alert_level(score, warn=0.1, alert=0.3):\n",
    "    if score < warn:\n",
    "        return \"NORMAL\"\n",
    "    elif score < alert:\n",
    "        return \"WARNING\"\n",
    "    else:\n",
    "        return \"ALERT\"\n",
    "\n",
    "\n",
    "for t in range(min(30, len(stream_df))):\n",
    "    row = stream_df.loc[t].values.tolist()\n",
    "    s = score_one_frame(model, scaler, row)\n",
    "\n",
    "    print(f\"[t={t:05d}] score={s:.5f} | {alert_level(s)}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Notes (important, practical)\n",
    "\n",
    "### 1) This baseline model scores each second correctly\n",
    "\n",
    "It outputs a score per row (per second).\n",
    "\n",
    "### 2) Isolation Forest is row-based\n",
    "\n",
    "It does not “understand sequences”.\n",
    "Your second model will handle patterns.\n",
    "\n",
    "### 3) The scaler is global\n",
    "\n",
    "This is the correct way if you want consistent scoring.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can also give you the **same notebook but with clean saving of metadata** like:\n",
    "\n",
    "* feature order\n",
    "* scaler version\n",
    "* training rows per patient\n",
    "  (so inference never breaks later).\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
